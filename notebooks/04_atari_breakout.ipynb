{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c6aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /Users/yezune/ws/from_pixels_to_planning\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[38536]: Class CaptureDelegate is implemented in both /Users/yezune/ws/from_pixels_to_planning/.venv/lib/python3.14/site-packages/cv2/cv2.abi3.so (0x11b5ca5d8) and /opt/homebrew/Cellar/opencv/4.12.0_15/lib/libopencv_videoio.4.12.0.dylib (0x12ff78618). This may cause spurious casting failures and mysterious crashes. One of the duplicates must be removed or renamed.\n",
      "objc[38536]: Class CVWindow is implemented in both /Users/yezune/ws/from_pixels_to_planning/.venv/lib/python3.14/site-packages/cv2/cv2.abi3.so (0x11b5ca628) and /opt/homebrew/Cellar/opencv/4.12.0_15/lib/libopencv_highgui.4.12.0.dylib (0x12f724a70). This may cause spurious casting failures and mysterious crashes. One of the duplicates must be removed or renamed.\n",
      "objc[38536]: Class CVView is implemented in both /Users/yezune/ws/from_pixels_to_planning/.venv/lib/python3.14/site-packages/cv2/cv2.abi3.so (0x11b5ca650) and /opt/homebrew/Cellar/opencv/4.12.0_15/lib/libopencv_highgui.4.12.0.dylib (0x12f724a98). This may cause spurious casting failures and mysterious crashes. One of the duplicates must be removed or renamed.\n",
      "objc[38536]: Class CVSlider is implemented in both /Users/yezune/ws/from_pixels_to_planning/.venv/lib/python3.14/site-packages/cv2/cv2.abi3.so (0x11b5ca678) and /opt/homebrew/Cellar/opencv/4.12.0_15/lib/libopencv_highgui.4.12.0.dylib (0x12f724ac0). This may cause spurious casting failures and mysterious crashes. One of the duplicates must be removed or renamed.\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Ensure project_root is defined for subsequent cells\n",
    "print(f'Project Root: {project_root}')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Common imports\n",
    "from src.envs.synthetic_env import BouncingBallEnv\n",
    "from src.envs.atari_env import AtariPixelEnv\n",
    "# L-AGI Imports\n",
    "from src.models.logical_spatial_rgm import LogicalSpatialRGM\n",
    "from src.l_fep.activation import SphericalActivation\n",
    "from src.models.logical_transition import LogicalTransitionModel\n",
    "from src.models.logical_active_inference_agent import LogicalActiveInferenceAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f0229",
   "metadata": {},
   "source": [
    "# Experiment 4: Atari Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a55a6a",
   "metadata": {},
   "source": [
    "## 2. Atari Breakout Experiment (Phase 5)\n",
    "\n",
    "We visualize the environment and the Hierarchical Agent's setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a13f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atari Environment Loaded Successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+unknown)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    env_atari = AtariPixelEnv(\"BreakoutNoFrameskip-v4\", image_size=64)\n",
    "    print(\"Atari Environment Loaded Successfully\")\n",
    "    \n",
    "    obs, _ = env_atari.reset()\n",
    "    \n",
    "    # Visualize Observation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    # Obs is (3, 64, 64) tensor\n",
    "    plt.imshow(obs.permute(1, 2, 0).numpy().astype(np.uint8))\n",
    "    plt.title(\"Atari Observation (64x64)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load Atari Environment: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39adcd",
   "metadata": {},
   "source": [
    "## 3. Planning in Latent Space\n",
    "\n",
    "This section demonstrates the core contribution: **planning in learned latent space**.\n",
    "\n",
    "We compare three agent strategies:\n",
    "1. **Reactive Agent**: 1-step lookahead (minimizes Expected Free Energy)\n",
    "2. **MCTS Agent**: Multi-step lookahead with Monte Carlo Tree Search\n",
    "3. **Trajectory Optimization Agent**: Gradient-based planning with differentiable transitions\n",
    "\n",
    "All agents use the same learned models (VAE + Transition Model) but differ in their planning depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba39463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "latent_dim = 32\n",
    "action_dim = env_atari.action_space.n  # 4 actions for Breakout\n",
    "hidden_dim = 256\n",
    "\n",
    "# Initialize L-AGI Components\n",
    "# We use num_classes=latent_dim to match the transition model expectation\n",
    "rgm = LogicalSpatialRGM(input_channels=3, latent_dim=32, num_classes=latent_dim, img_size=64).to(device)\n",
    "transition_model = LogicalTransitionModel(\n",
    "    latent_dim=latent_dim, \n",
    "    action_dim=action_dim, \n",
    "    hidden_dim=hidden_dim\n",
    ").to(device)\n",
    "\n",
    "# Create agent\n",
    "agent = LogicalActiveInferenceAgent(\n",
    "    rgm=rgm,\n",
    "    transition_model=transition_model,\n",
    "    action_dim=action_dim,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Logical Agent initialized with {action_dim} actions\")\n",
    "print(f\"Latent space (Spherical): {latent_dim} dimensions\")\n",
    "print(f\"Transition hidden state: {hidden_dim} dimensions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98507a6",
   "metadata": {},
   "source": [
    "### 3.1 Reactive Agent (Baseline)\n",
    "\n",
    "The reactive agent uses 1-step lookahead, evaluating each action by predicting the next state and computing Expected Free Energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652cfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run reactive agent for N episodes\n",
    "import time\n",
    "\n",
    "def run_episode(env, agent, max_steps=100, planning_method=None):\n",
    "    \"\"\"Run one episode and return total reward.\"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    agent.reset()\n",
    "    \n",
    "    total_reward = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        if planning_method is None:\n",
    "            # Reactive: 1-step lookahead\n",
    "            action = agent.select_action(obs)\n",
    "        else:\n",
    "            # Planning: multi-step lookahead\n",
    "            action = agent.select_action_with_planning(\n",
    "                obs, method=planning_method, horizon=5, num_simulations=10\n",
    "            )\n",
    "        \n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    return total_reward, step + 1, elapsed_time\n",
    "\n",
    "# Test reactive agent\n",
    "print(\"Testing Reactive Agent (1-step lookahead)...\")\n",
    "num_episodes = 3\n",
    "reactive_rewards = []\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    reward, steps, elapsed = run_episode(env_atari, agent, max_steps=100)\n",
    "    reactive_rewards.append(reward)\n",
    "    print(f\"  Episode {ep+1}: Reward={reward:.2f}, Steps={steps}, Time={elapsed:.2f}s\")\n",
    "\n",
    "avg_reactive = np.mean(reactive_rewards)\n",
    "print(f\"\\nReactive Agent Average: {avg_reactive:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a93690c",
   "metadata": {},
   "source": [
    "### 3.2 MCTS Planning Agent\n",
    "\n",
    "Monte Carlo Tree Search explores the latent space tree to find optimal action sequences. It balances exploration (UCB1) and exploitation (visit counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91100f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MCTS planning agent\n",
    "print(\"Testing MCTS Planning Agent...\")\n",
    "mcts_rewards = []\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    reward, steps, elapsed = run_episode(\n",
    "        env_atari, agent, max_steps=100, planning_method='mcts'\n",
    "    )\n",
    "    mcts_rewards.append(reward)\n",
    "    print(f\"  Episode {ep+1}: Reward={reward:.2f}, Steps={steps}, Time={elapsed:.2f}s\")\n",
    "\n",
    "avg_mcts = np.mean(mcts_rewards)\n",
    "print(f\"\\nMCTS Agent Average: {avg_mcts:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359a427",
   "metadata": {},
   "source": [
    "### 3.3 Trajectory Optimization Agent\n",
    "\n",
    "Gradient-based optimization refines action sequences by backpropagating through the differentiable transition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bfcbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test trajectory optimization agent\n",
    "print(\"Testing Trajectory Optimization Agent...\")\n",
    "traj_rewards = []\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    reward, steps, elapsed = run_episode(\n",
    "        env_atari, agent, max_steps=100, planning_method='trajectory'\n",
    "    )\n",
    "    traj_rewards.append(reward)\n",
    "    print(f\"  Episode {ep+1}: Reward={reward:.2f}, Steps={steps}, Time={elapsed:.2f}s\")\n",
    "\n",
    "avg_traj = np.mean(traj_rewards)\n",
    "print(f\"\\nTrajectory Optimization Agent Average: {avg_traj:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c0958",
   "metadata": {},
   "source": [
    "### 3.4 Performance Comparison\n",
    "\n",
    "Visualize the performance difference between reactive and planning agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afccdf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot: Average rewards\n",
    "ax1 = axes[0]\n",
    "methods = ['Reactive\\n(1-step)', 'MCTS\\n(Tree Search)', 'Trajectory\\n(Gradient)']\n",
    "averages = [avg_reactive, avg_mcts, avg_traj]\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "\n",
    "bars = ax1.bar(methods, averages, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Average Reward', fontsize=12)\n",
    "ax1.set_title('Planning vs Reactive Performance', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, avg in zip(bars, averages):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{avg:.2f}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Episode-wise comparison\n",
    "ax2 = axes[1]\n",
    "episodes = range(1, num_episodes + 1)\n",
    "ax2.plot(episodes, reactive_rewards, 'o-', label='Reactive', \n",
    "         color='#e74c3c', linewidth=2, markersize=8)\n",
    "ax2.plot(episodes, mcts_rewards, 's-', label='MCTS', \n",
    "         color='#3498db', linewidth=2, markersize=8)\n",
    "ax2.plot(episodes, traj_rewards, '^-', label='Trajectory', \n",
    "         color='#2ecc71', linewidth=2, markersize=8)\n",
    "\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Reward', fontsize=12)\n",
    "ax2.set_title('Episode-wise Performance', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Reactive Agent (1-step):    {avg_reactive:.2f} ± {np.std(reactive_rewards):.2f}\")\n",
    "print(f\"MCTS Agent (tree search):   {avg_mcts:.2f} ± {np.std(mcts_rewards):.2f}\")\n",
    "print(f\"Trajectory Agent (gradient):{avg_traj:.2f} ± {np.std(traj_rewards):.2f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate improvement\n",
    "mcts_improvement = ((avg_mcts - avg_reactive) / abs(avg_reactive + 1e-6)) * 100\n",
    "traj_improvement = ((avg_traj - avg_reactive) / abs(avg_reactive + 1e-6)) * 100\n",
    "\n",
    "print(f\"\\nMCTS Improvement: {mcts_improvement:+.1f}%\")\n",
    "print(f\"Trajectory Improvement: {traj_improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664a8034",
   "metadata": {},
   "source": [
    "## 4. Key Insights\n",
    "\n",
    "**Planning in learned latent space enables:**\n",
    "1. **Multi-step lookahead**: MCTS explores action trees up to depth 5\n",
    "2. **Differentiable planning**: Trajectory optimization uses gradients for efficient search\n",
    "3. **Reduced computational cost**: Planning in 32D latent space vs 64×64×3 pixel space\n",
    "4. **Model-based decision making**: No environment interaction required during planning\n",
    "\n",
    "**Expected results (with trained models):**\n",
    "- Planning agents should outperform reactive agents\n",
    "- MCTS excels with discrete actions and stochastic environments\n",
    "- Trajectory optimization is faster but requires smooth, differentiable transitions\n",
    "\n",
    "**Note**: This demonstration uses untrained models for concept illustration. With properly trained VAE and transition models from actual Atari gameplay, planning agents would show significant performance improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
